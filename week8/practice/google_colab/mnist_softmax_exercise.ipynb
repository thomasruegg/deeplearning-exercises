{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlmN_abp4vVb"
      },
      "source": [
        "# Lab 8\n",
        "## Implementing the Softmax Regression\n",
        "In this lab we will apply softmax regression to the MNIST data.\n",
        "\n",
        "In a first step let’s import all needed packages, load and prepare the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmqCy7HK4vVc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load MNIST data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVLv-OTD4vVd"
      },
      "source": [
        "### Prepare images\n",
        "The images come in matrix form of 28x28 uint8 values. Moreover, there exists no\n",
        "validation data split. Therefore, we first convert the images into arrays of shape (784,)\n",
        "and datatype float32 whose values lie between 0.0 and 1.0. Additionally, we split off a validation dataset from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgSXJ83Z4vVd"
      },
      "outputs": [],
      "source": [
        "# Scale images\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Flatten images\n",
        "x_train = x_train.reshape([len(x_train), -1]).astype(\"float32\")\n",
        "x_test = x_test.reshape([len(x_test), -1]).astype(\"float32\")\n",
        "\n",
        "# Split off validation dataset from training dataset\n",
        "indices = np.random.choice(len(y_train), 5000, replace=False)\n",
        "x_valid = x_train[indices, :]\n",
        "y_valid = y_train[indices]\n",
        "x_train = np.delete(x_train, indices, axis=0)\n",
        "y_train = np.delete(y_train, indices, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbyu6Wrr4vVe"
      },
      "source": [
        "### Create Datasets\n",
        "Next we have to convert the image labels (y_train, y_valid and y_test) into one-\n",
        "hot vectors as described in subsection 2.1. All three splits are then converted to a\n",
        "tf.data.Dataset which allows us to apply some preprocessing operations (such as\n",
        "batching and shuffling) and lets us efficiently loop over the dataset split. For more infor-\n",
        "mation about TensorFlow input pipelines see [here](https://www.tensorflow.org/guide/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7_2xHvS4vVe"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot tensor\n",
        "y_train = tf.one_hot(y_train, 10)\n",
        "y_test = tf.one_hot(y_test, 10)\n",
        "y_valid = tf.one_hot(y_valid, 10)\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 32\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(len(y_train), reshuffle_each_iteration=True)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy5S-aSw4vVf"
      },
      "source": [
        "### Define Model\n",
        "As you can see in the above code fragment, here we use a mini-batch size of 32.\n",
        "Now we finally get to build our softmax regression model. In contrast to TensorFlow 1,\n",
        "in TensorFlow 2 it is very simple to define a model. In fact, there are many different ways\n",
        "6\n",
        "to define a model in TensorFlow 2 (see [here](https://www.tensorflow.org/guide/keras/sequential_model) for more information). Here we will use the\n",
        "subclassing method, whereby we inherit from the class tf.keras.Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6FM2MSd4vVf"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(10, use_bias=True)\n",
        "        self.s1 = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.d1(x)\n",
        "        return self.s1(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dIv5HOS4vVg"
      },
      "source": [
        "Thanks to this inheritance concept we can use the methods of the parent class tf.keras.Model\n",
        "(you might know this concept from Python programming). In the class constructor\n",
        "__init__() we define all the layers which we need in order to build the model. In\n",
        "case of softmax regression we need a Dense layer (for the matix multiplication) and a\n",
        "Softmax layer (for probability conversion). These methods are then invoked in the cor-\n",
        "rect order in the call method of the same class. Next we instantiate an object of this\n",
        "child class. Note that call( ) is an overridden parent class method which is called\n",
        "when the instance model is called."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4gLevqk4vVg"
      },
      "source": [
        "### Define Loss, Optimizer and Metrics\n",
        "Next we need to define a loss function (cross-entropy in our case) and a way to minimize\n",
        "this loss function (Stochastic gradient decent with a learning-rate of 0.01 in our case).\n",
        "Furthermore, we will instantiate two metric classes for each dataset split, which lets us\n",
        "easily accumulate the results over all iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwVb1RlN4vVh"
      },
      "outputs": [],
      "source": [
        "# Choose an optimizer and loss function for training:\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Select metrics to measure the loss and the accuracy of the model.\n",
        "# These metrics accumulate the values over epochs and then print the overall result.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
        "valid_accuracy = tf.keras.metrics.CategoricalAccuracy(name='valid_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtrYCiGo4vVh"
      },
      "source": [
        "### Training, Validation and Testing step\n",
        "The training, validation and testing process is performed with mini-batches of 32 data\n",
        "points. In a next step we have to specify the operations which need to be carried out on\n",
        "such a mini-batch.\n",
        "A training step looks similar to what we’ve seen in last week’s lab. We have to calculate\n",
        "the gradients which are subtracted from all the trainable variables of our model. Addi-\n",
        "tionally, we accumulate the loss and accuracy value of each training step by calling the\n",
        "7\n",
        "train_loss and train_accuracy object respectively.\n",
        "In a validation and testing step we simply calculate the loss and accuracy and accumulate\n",
        "the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzwwJeyX4vVi"
      },
      "outputs": [],
      "source": [
        "# Use tf.GradientTape to train the model\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # training=True is only needed if there are layers with different\n",
        "        # behavior during training versus inference (e.g. Dropout).\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "# Validate the model\n",
        "@tf.function\n",
        "def valid_step(images, labels):\n",
        "    # training=False is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    valid_loss(t_loss)\n",
        "    valid_accuracy(labels, predictions)\n",
        "\n",
        "# Test the model\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    # training=False is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBUKdhEk4vVi"
      },
      "source": [
        "### Training\n",
        "Now we finally get to train, validate and test our model. Specifically, we train and validate\n",
        "our model for 25 epochs (in each epoch we run over the whole training dataset and\n",
        "validation dataset). At the end we test whether our model also generalizes to previously\n",
        "unseen data points by evaluating the loss and accuracy of the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpfxZu184vVi"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 25\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "    valid_loss.reset_state()\n",
        "    valid_accuracy.reset_state()\n",
        "\n",
        "    for images, labels in train_ds:\n",
        "        train_step(images, labels)\n",
        "\n",
        "    for valid_images, valid_labels in valid_ds:\n",
        "        valid_step(valid_images, valid_labels)\n",
        "\n",
        "    print(\n",
        "        \"Epoch {:2d}: \".format(epoch + 1),\n",
        "        \"Train Loss: {:3.3f}, \".format(train_loss.result()),\n",
        "        \"Train Accuracy: {:3.3f}%, \".format(train_accuracy.result() * 100),\n",
        "        \"Validation Loss: {:3.3f}, \".format(valid_loss.result()),\n",
        "        \"Validation Accuracy: {:3.3f}%\".format(valid_accuracy.result() * 100),\n",
        "    )\n",
        "\n",
        "# Test resulting classifier\n",
        "test_loss.reset_state()\n",
        "test_accuracy.reset_state()\n",
        "for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "print(\n",
        "    \"\\nTesting result: \",\n",
        "    \"Test Loss: {:3.3f}, \".format(test_loss.result()),\n",
        "    \"Test Accuracy: {:3.3f}%\".format(test_accuracy.result() * 100),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}