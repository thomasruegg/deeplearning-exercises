{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12 (Solution)\n",
    "## Convolutional Neural Network\n",
    "\n",
    "Your task is to implement the convolutional neural network below according to the architecture described in the PDF. Feel free to define the model as it suits you. Use the following TensorFlow tutorials as a source of inspiration:\n",
    "\n",
    "- https://www.tensorflow.org/guide/keras/sequential_model\n",
    "- https://www.tensorflow.org/guide/keras/functional\n",
    "- https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "\n",
    "If you correctly implement the CNN you should be able to achieve an accuracy of approximately 99.5% on the test dataset. Hint: Keep in mind that the dropout behaves differently during training than at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Softmax,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    ReLU,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The following class handles the data loading / preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderMNIST:\n",
    "    \"\"\"\n",
    "    Prepare TensorFlow iterator of MNIST dataset and split data into train,\n",
    "    valid, and test subsets.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flatten=True):\n",
    "        VALIDATION_DATASET_SIZE = 5000\n",
    "        MINI_BATCH_SIZE = 32\n",
    "\n",
    "        # Load MNIST data\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # Scale images\n",
    "        x_train = x_train / 255.0\n",
    "        x_test = x_test / 255.0\n",
    "\n",
    "        # Flatten images\n",
    "        if flatten:\n",
    "            x_train = x_train.reshape([len(x_train), -1])\n",
    "            x_test = x_test.reshape([len(x_test), -1])\n",
    "        else:\n",
    "            x_train = x_train.reshape(x_train.shape + (1,))\n",
    "            x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "        # Cast dtype\n",
    "        x_train = x_train.astype(\"float32\")\n",
    "        x_test = x_test.astype(\"float32\")\n",
    "\n",
    "        # Split off validation dataset from training dataset\n",
    "        indices = np.random.choice(len(y_train), VALIDATION_DATASET_SIZE, replace=False)\n",
    "        x_valid = x_train[indices, :]\n",
    "        y_valid = y_train[indices]\n",
    "        x_train = np.delete(x_train, indices, axis=0)\n",
    "        y_train = np.delete(y_train, indices, axis=0)\n",
    "\n",
    "        # Convert labels to one-hot tensor\n",
    "        y_train = tf.one_hot(y_train, 10)\n",
    "        y_test = tf.one_hot(y_test, 10)\n",
    "        y_valid = tf.one_hot(y_valid, 10)\n",
    "\n",
    "        # Create datasets\n",
    "        self._train_dataset = (\n",
    "            tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            .shuffle(len(y_train))\n",
    "            .batch(MINI_BATCH_SIZE)\n",
    "        )\n",
    "        self._valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (x_valid, y_valid)\n",
    "        ).batch(MINI_BATCH_SIZE)\n",
    "        self._test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "            MINI_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self._train_dataset\n",
    "\n",
    "    @property\n",
    "    def valid_dataset(self):\n",
    "        return self._valid_dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self._test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This class defines the neural network as derived tf.keras.Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    \"\"\"\n",
    "    TensorFlow convolutional neural network which is used as classifier for\n",
    "    MNIST images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string, optional\n",
    "        Name of model. The default is None.\n",
    "    **kwargs :\n",
    "        See description of tf.keras.Model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super(MyModel, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        self.conv_1 = Conv2D(filters=32, kernel_size=5, padding=\"same\")\n",
    "        self.conv_2 = Conv2D(filters=64, kernel_size=5, padding=\"same\")\n",
    "\n",
    "        self.relu_1 = ReLU()\n",
    "        self.relu_2 = ReLU()\n",
    "        self.relu_3 = ReLU()\n",
    "\n",
    "        self.pool_1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2))\n",
    "\n",
    "        self.flat_1 = Flatten()\n",
    "\n",
    "        self.drop_1 = Dropout(0.5)\n",
    "        self.drop_2 = Dropout(0.5)\n",
    "\n",
    "        self.dense_1 = Dense(1024)\n",
    "        self.dense_2 = Dense(10)\n",
    "\n",
    "        self.softm_1 = Softmax()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass of MyModel with specific input x.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor float32 (None, 28, 28, 1)\n",
    "            Input to MyModel.\n",
    "        training : bool, optional\n",
    "            training=True is only needed if there are layers with different\n",
    "            behavior during training versus inference (e.g. Dropout).\n",
    "            The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : tensor float32 (None, 10)\n",
    "            Output of MyModel.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # First convolutional stage\n",
    "        t_conv_1 = self.conv_1(x)\n",
    "        t_relu_1 = self.relu_1(t_conv_1)\n",
    "        t_pool_1 = self.pool_1(t_relu_1)\n",
    "\n",
    "        # Second convolutional stage\n",
    "        t_conv_2 = self.conv_2(t_pool_1)\n",
    "        t_relu_2 = self.relu_2(t_conv_2)\n",
    "        t_pool_2 = self.pool_2(t_relu_2)\n",
    "\n",
    "        # Flatten\n",
    "        t_flat_1 = self.flat_1(t_pool_2)\n",
    "\n",
    "        # First dense stage\n",
    "        t_drop_1 = self.drop_1(t_flat_1, training)\n",
    "        t_dens_1 = self.dense_1(t_drop_1)\n",
    "        t_relu_3 = self.relu_3(t_dens_1)\n",
    "\n",
    "        # Second dense stage\n",
    "        t_drop_2 = self.drop_2(t_relu_3, training)\n",
    "        t_dens_2 = self.dense_2(t_drop_2)\n",
    "        out = self.softm_1(t_dens_2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCounter:\n",
    "    def __init__(self, patience=3, improvement_margin=0.0002):\n",
    "        self._patience = patience\n",
    "        self._improvement_margin = improvement_margin\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._best = 0.0\n",
    "        self._count = 0\n",
    "\n",
    "    def update(self, current):\n",
    "        if current < self._best + self._improvement_margin:\n",
    "            self._count += 1\n",
    "        else:\n",
    "            self._count = 0\n",
    "            self._best = current\n",
    "\n",
    "    def is_stopping_criteria_reached(self):\n",
    "        return self._count > self._patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "This class executes the training, validation and testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \"\"\"\n",
    "    Training class, for model, logging metrics in tensorboard_dir.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        TensorFlow model to be trained.\n",
    "    tensorboard_dir : string\n",
    "        Path to tensorboard directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tensorboard_dir):\n",
    "        self._model = model\n",
    "        self._optimizer = tf.keras.optimizers.Adam()\n",
    "        self._loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self._init_metrics()\n",
    "        self._init_tensorboard(tensorboard_dir)\n",
    "        self._early_stopping_counter = EarlyStoppingCounter()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "        test_dataset,\n",
    "        max_epochs=50,\n",
    "        evaluate_every=5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This method implements the training loop using train_dataset and\n",
    "        evaluetes the valid_dataset every evaluate_every epochs. At the end\n",
    "        it tests the classifiers performance based on test_dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_dataset : TF Dataset\n",
    "            Training dataset which consists of pairs of input and labels.\n",
    "        valid_dataset : TF Dataset\n",
    "            Validation dataset which consists of pairs of input and labels.\n",
    "        test_dataset : TF Dataset\n",
    "            Test dataset which consists of pairs of input and labels.\n",
    "        max_epochs : int, optional\n",
    "            Maximum number of times train_dataset is iterated over.\n",
    "            The default is 50.\n",
    "        evaluate_every : int, optional\n",
    "            Every evaluate_every'th epoch the model is evaluated based on\n",
    "            valid_dataset. The default is 5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            self._reset_train_metrics()\n",
    "            for images, labels in train_dataset:\n",
    "                self._train_step(images, labels)\n",
    "\n",
    "            if epoch % evaluate_every == 0:\n",
    "                self._reset_valid_metrics()\n",
    "                for images, labels in valid_dataset:\n",
    "                    self._valid_step(images, labels)\n",
    "\n",
    "                self._print_train_and_valid_results(epoch, max_epochs)\n",
    "                self._update_tensorboard(epoch)\n",
    "\n",
    "                self._early_stopping_counter.update(self._valid_accuracy.result())\n",
    "                if self._early_stopping_counter.is_stopping_criteria_reached():\n",
    "                    print(\"Early stopping at epoch {:2d}.\".format(epoch))\n",
    "                    self._early_stopping_counter.reset()\n",
    "                    break\n",
    "\n",
    "        self._reset_test_metrics()\n",
    "        for images, labels in test_dataset:\n",
    "            self._test_step(images, labels)\n",
    "\n",
    "        self._print_test_results()\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(self, images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self._model(images, training=True)\n",
    "            loss = self._loss(labels, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self._model.trainable_variables)\n",
    "        self._optimizer.apply_gradients(zip(gradients, self._model.trainable_variables))\n",
    "        self._update_train_metrics(loss, labels, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def _valid_step(self, images, labels):\n",
    "        predictions = self._model(images, training=False)\n",
    "        loss = self._loss(labels, predictions)\n",
    "        self._update_valid_metrics(loss, labels, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def _test_step(self, images, labels):\n",
    "        predictions = self._model(images, training=False)\n",
    "        loss = self._loss(labels, predictions)\n",
    "        self._update_test_metrics(loss, labels, predictions)\n",
    "\n",
    "    def _init_metrics(self):\n",
    "        self._init_train_metrics()\n",
    "        self._init_valid_metrics()\n",
    "        self._init_test_metrics()\n",
    "\n",
    "    def _init_train_metrics(self):\n",
    "        self._train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self._train_accuracy = tf.keras.metrics.CategoricalAccuracy(\n",
    "            name=\"train_accuracy\"\n",
    "        )\n",
    "\n",
    "    def _init_valid_metrics(self):\n",
    "        self._valid_loss = tf.keras.metrics.Mean(name=\"valid_loss\")\n",
    "        self._valid_accuracy = tf.keras.metrics.CategoricalAccuracy(\n",
    "            name=\"valid_accuracy\"\n",
    "        )\n",
    "\n",
    "    def _init_test_metrics(self):\n",
    "        self._test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
    "        self._test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "\n",
    "    def _reset_train_metrics(self):\n",
    "        self._train_loss.reset_state()\n",
    "        self._train_accuracy.reset_state()\n",
    "\n",
    "    def _reset_valid_metrics(self):\n",
    "        self._valid_loss.reset_state()\n",
    "        self._valid_accuracy.reset_state()\n",
    "\n",
    "    def _reset_test_metrics(self):\n",
    "        self._test_loss.reset_state()\n",
    "        self._test_accuracy.reset_state()\n",
    "\n",
    "    def _update_train_metrics(self, loss, labels, predictions):\n",
    "        self._train_loss(loss)\n",
    "        self._train_accuracy(labels, predictions)\n",
    "\n",
    "    def _update_valid_metrics(self, loss, labels, predictions):\n",
    "        self._valid_loss(loss)\n",
    "        self._valid_accuracy(labels, predictions)\n",
    "\n",
    "    def _update_test_metrics(self, loss, labels, predictions):\n",
    "        self._test_loss(loss)\n",
    "        self._test_accuracy(labels, predictions)\n",
    "\n",
    "    def _print_train_and_valid_results(self, epoch, max_epochs):\n",
    "        print(\n",
    "            \"Epoch {:3d} of {:3d}, \".format(epoch, max_epochs),\n",
    "            \"Train Loss: {:3.3f}, \".format(self._train_loss.result()),\n",
    "            \"Train Accuracy: {:3.3f}%, \".format(self._train_accuracy.result() * 100),\n",
    "            \"Valid Loss: {:3.3f}, \".format(self._valid_loss.result()),\n",
    "            \"Valid Accuracy: {:3.3f}%\".format(self._valid_accuracy.result() * 100),\n",
    "        )\n",
    "\n",
    "    def _print_test_results(self):\n",
    "        print(\n",
    "            \"\\nTest Loss: {:3.3f}, \".format(self._test_loss.result()),\n",
    "            \"Test Accuracy: {:3.3f}%\".format(self._test_accuracy.result() * 100),\n",
    "        )\n",
    "\n",
    "    def _init_tensorboard(self, tensorboard_dir):\n",
    "        self._train_summary_writer = tf.summary.create_file_writer(\n",
    "            os.path.join(tensorboard_dir, \"train\")\n",
    "        )\n",
    "        self._valid_summary_writer = tf.summary.create_file_writer(\n",
    "            os.path.join(tensorboard_dir, \"valid\")\n",
    "        )\n",
    "\n",
    "    def _update_tensorboard(self, epoch):\n",
    "        with self._train_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Train loss\", self._train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"Train accuracy\", self._train_accuracy.result(), step=epoch\n",
    "            )\n",
    "\n",
    "        with self._valid_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Valid loss\", self._valid_loss.result(), step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"Valid accuracy\", self._valid_accuracy.result(), step=epoch\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "In the following cell all objects are created and the training process is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_PATH = \"./logs\"\n",
    "\n",
    "data_loader = DataLoaderMNIST(flatten=False)\n",
    "model = MyModel(\"MNISTClassifier\")\n",
    "train = Training(model, TENSORBOARD_PATH)\n",
    "\n",
    "train_dataset = data_loader.train_dataset\n",
    "valid_dataset = data_loader.valid_dataset\n",
    "test_dataset = data_loader.test_dataset\n",
    "\n",
    "train(train_dataset, valid_dataset, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMAVrEEuyfZENjjKjv9B8w2",
   "name": "convolutional_neural_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
