{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13\n",
    "## Recurrent Neural Networks\n",
    "In this weekâ€™s lab we will apply a bidirectional RNN (GRU) to a binary text classification problem.   Specifically, we will train a model to distinguish between positive and negative sentiment of movie reviews using the [IBM movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/). If you are not familiar with bidirectional RNNs, see Chapter 10.3 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, GRU, Dense\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The following class handles the data loading / preprocessing part.\n",
    "\n",
    "First we download the whole dataset and divide it into a train, valid, test and unsupervised subset.\n",
    "The unsupervised dataset is a subset of movie reviews which is not labeled.\n",
    "We will need this to initialize the text-to-number encoder later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()\n",
    "\n",
    "\n",
    "class DataLoaderIMDB:\n",
    "    \"\"\"\n",
    "    This class downloads and prepares the IMDB large movie dataset.\n",
    "    It splits the dataset into train, valid, test, and unsupervised subsets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        MINI_BATCH_SIZE = 64\n",
    "        VALID_SPLIT = 0.1\n",
    "\n",
    "        dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "        valid_size = int(info.splits[\"train\"].num_examples * VALID_SPLIT)\n",
    "\n",
    "        self._valid_dataset = dataset[\"train\"].take(valid_size)\n",
    "        self._train_dataset = dataset[\"train\"].skip(valid_size)\n",
    "        self._test_dataset = dataset[\"test\"]\n",
    "        self._unsupervised_dataset = dataset[\"unsupervised\"]\n",
    "\n",
    "        # example = next(self._train_dataset.as_numpy_iterator())\n",
    "        # print(\"text:\", example[0])\n",
    "        # print(\"label:\", example[1])\n",
    "\n",
    "        self._train_dataset = self._train_dataset.shuffle(10000)\n",
    "\n",
    "        # Batch\n",
    "        self._train_dataset = self._train_dataset.batch(MINI_BATCH_SIZE)\n",
    "        self._valid_dataset = self._valid_dataset.batch(MINI_BATCH_SIZE)\n",
    "        self._test_dataset = self._test_dataset.batch(MINI_BATCH_SIZE)\n",
    "        self._unsupervised_dataset = self._unsupervised_dataset.batch(MINI_BATCH_SIZE)\n",
    "\n",
    "        # Prefetch\n",
    "        self._train_dataset = self._train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        self._valid_dataset = self._valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        self._test_dataset = self._test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        self._unsupervised_dataset = self._unsupervised_dataset.prefetch(\n",
    "            tf.data.AUTOTUNE\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self._train_dataset\n",
    "\n",
    "    @property\n",
    "    def valid_dataset(self):\n",
    "        return self._valid_dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self._test_dataset\n",
    "\n",
    "    @property\n",
    "    def unsupervised_dataset(self):\n",
    "        return self._unsupervised_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "This class defines the neural network as base class of tf.keras.Model.\n",
    "\n",
    "Each element of our dataset consists of a string of words (one movie review per element).\n",
    "However, neural networks do not know how to process letters or sentences.\n",
    "Therefore, we first have to convert each word into a vector using a TextVectorizer in combination with an Embedding layer.\n",
    "\n",
    "- The TextVectorizer simply converts the text to a sequence of token indices (basically a lookup table which maps from word to integer).\n",
    "- An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors. If you want to know more about embedding layers, see [here](http://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "To complete the movie review classifier, we add a bidirectional RNN on top of these preprocessing layers.\n",
    "The concatenated final states of the two RNNs are then used as an input for two feed-forward network layers with output size of 64 and 1, respectively.\n",
    "\n",
    "The following cell shows one possible way to define the RNN model. There are many other ways to define such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    \"\"\"\n",
    "    RNN for text classification of movie reviews.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : ndarray (None,)\n",
    "        Training vocabulary for text vectorizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "\n",
    "        self.enc_1 = TextVectorization(max_tokens=1000)\n",
    "        self.enc_1.adapt(vocabulary)\n",
    "        self.emb_1 = Embedding(\n",
    "            input_dim=len(self.enc_1.get_vocabulary()), output_dim=64, mask_zero=True\n",
    "        )\n",
    "        self.bid_1 = Bidirectional(GRU(64))\n",
    "        self.dense_1 = Dense(64, activation=\"relu\")\n",
    "        self.dense_2 = Dense(1)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray (None,)\n",
    "            Mini-batch of byte strings which corresponds to RNN input\n",
    "        training : bool, optional\n",
    "            Training or testing mode, by default False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor float32 (None, 1)\n",
    "            Logits of classifier (pre-sigmoid activations)\n",
    "        \"\"\"\n",
    "        t_enc_1 = self.enc_1(x)\n",
    "        t_emb_1 = self.emb_1(t_enc_1)\n",
    "        t_bid_1 = self.bid_1(t_emb_1)\n",
    "        t_dense_1 = self.dense_1(t_bid_1)\n",
    "        out = self.dense_2(t_dense_1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_PATH = \"./logs\"\n",
    "\n",
    "data_loader = DataLoaderIMDB()\n",
    "\n",
    "train_dataset = data_loader.train_dataset\n",
    "valid_dataset = data_loader.valid_dataset\n",
    "test_dataset = data_loader.test_dataset\n",
    "unsupervised_dataset = data_loader.unsupervised_dataset\n",
    "\n",
    "# Create vocabulary for text vectorizer\n",
    "vocab_1 = train_dataset.map(lambda text, label: text)\n",
    "vocab_2 = unsupervised_dataset.map(lambda text, label: text)\n",
    "vocabulary = vocab_1.concatenate(vocab_2)\n",
    "\n",
    "model = MyModel(vocabulary)\n",
    "\n",
    "# Compile the Keras model to configure the training process\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=TENSORBOARD_PATH, write_graph=False\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "# Test model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
