{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IMajkLTPzhl"
      },
      "source": [
        "# Lab 10 / 11\n",
        "## Deep Feedforward Networks\n",
        "In lab 8 and 9 we developed a softmax regression model which can classify handwritten digits (MNIST dataset).\n",
        "\n",
        "First try to understand how this softmax regression model and the corresponding training is implemented in this jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTekhoDPzho"
      },
      "source": [
        "In its present form, the classifier achieves a performance of approximately 93 % on the test dataset. Try to increase the performance of the network by using some of the methods\n",
        "introduced in chapters 7 and 8 of the book.\n",
        "\n",
        "Originally, the code in this jupyter notebook was split into modules. Executing code in Google Colab, however, makes it difficult to import code from other jupyter notebooks. Please note that, unlike here, the training process of a neural network is normally split in several modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEVNyP6SPzhp"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3g73U9TPzhp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXnMJrpwPzhp"
      },
      "source": [
        "## Data Loading\n",
        "The following class handles the data loading / preprocessing part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J56jinqYPzhp"
      },
      "outputs": [],
      "source": [
        "class DataLoaderMNIST:\n",
        "    \"\"\"\n",
        "    Prepare TensorFlow iterator of MNIST dataset and split data into train,\n",
        "    valid, and test subsets.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        VALIDATION_DATASET_SIZE = 5000\n",
        "        MINI_BATCH_SIZE = 32\n",
        "\n",
        "        # Load MNIST data\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        # Scale images\n",
        "        x_train = x_train / 255.0\n",
        "        x_test = x_test / 255.0\n",
        "\n",
        "        # Flatten images\n",
        "        x_train = x_train.reshape([len(x_train), -1]).astype(\"float32\")\n",
        "        x_test = x_test.reshape([len(x_test), -1]).astype(\"float32\")\n",
        "\n",
        "        # Split off validation dataset from training dataset\n",
        "        indices = np.random.choice(len(y_train), VALIDATION_DATASET_SIZE, replace=False)\n",
        "        x_valid = x_train[indices, :]\n",
        "        y_valid = y_train[indices]\n",
        "        x_train = np.delete(x_train, indices, axis=0)\n",
        "        y_train = np.delete(y_train, indices, axis=0)\n",
        "\n",
        "        # Convert labels to one-hot tensor\n",
        "        y_train = tf.one_hot(y_train, 10)\n",
        "        y_test = tf.one_hot(y_test, 10)\n",
        "        y_valid = tf.one_hot(y_valid, 10)\n",
        "\n",
        "        # Create datasets\n",
        "        self._train_dataset = (\n",
        "            tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            .shuffle(len(y_train))\n",
        "            .batch(MINI_BATCH_SIZE)\n",
        "        )\n",
        "        self._valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (x_valid, y_valid)\n",
        "        ).batch(MINI_BATCH_SIZE)\n",
        "        self._test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "            MINI_BATCH_SIZE\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def train_dataset(self):\n",
        "        return self._train_dataset\n",
        "\n",
        "    @property\n",
        "    def valid_dataset(self):\n",
        "        return self._valid_dataset\n",
        "\n",
        "    @property\n",
        "    def test_dataset(self):\n",
        "        return self._test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpTq-ivwPzhq"
      },
      "source": [
        "## Model\n",
        "This class defines the neural network as derived tf.keras.Model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwccBqWzPzhq"
      },
      "outputs": [],
      "source": [
        "class MyModel(Model):\n",
        "    \"\"\"\n",
        "    TensorFlow neural network which is used as classifier for flattened\n",
        "    MNIST images.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    name : string, optional\n",
        "        Name of model. The default is None.\n",
        "    **kwargs :\n",
        "        See description of tf.keras.Model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name=None, **kwargs):\n",
        "\n",
        "        super(MyModel, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        self.d1 = Dense(10, use_bias=True, name=\"Dense_1\")\n",
        "        self.s1 = Softmax(name=\"Softmax_1\")\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        \"\"\"\n",
        "        Forward pass of MyModel with specific input x.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Tensor float32 (None, 784)\n",
        "            Input to MyModel.\n",
        "        training : bool, optional\n",
        "            training=True is only needed if there are layers with different\n",
        "            behavior during training versus inference (e.g. Dropout).\n",
        "            The default is False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : tensor float32 (None, 10)\n",
        "            Output of MyModel.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.d1(x)\n",
        "        out = self.s1(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mTcmBruUoY5"
      },
      "source": [
        "## Early Stopping\n",
        "This class defines a counter for early stopping."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStoppingCounter:\n",
        "    def __init__(self, patience=0, improvement_margin=0.0):\n",
        "        self._patience = patience\n",
        "        self._improvement_margin = improvement_margin\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self._best = 0.0\n",
        "        self._count = 0\n",
        "\n",
        "    def update(self, current):\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def is_stopping_criteria_reached(self):\n",
        "        # TODO\n",
        "        return False"
      ],
      "metadata": {
        "id": "nji-7dKAU-RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qjTVghqPzhr"
      },
      "source": [
        "## Training\n",
        "This class executes the training, validation and testing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHENBncRPzhs"
      },
      "outputs": [],
      "source": [
        "class Training:\n",
        "    \"\"\"\n",
        "    Training class, for model, logging metrics in tensorboard_dir.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : tf.keras.Model\n",
        "        TensorFlow model to be trained.\n",
        "    tensorboard_dir : string\n",
        "        Path to tensorboard directory.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tensorboard_dir):\n",
        "        self._model = model\n",
        "        self._optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "        self._loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "        self._init_metrics()\n",
        "        self._init_tensorboard(tensorboard_dir)\n",
        "        self._early_stopping_counter = EarlyStoppingCounter()\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        train_dataset,\n",
        "        valid_dataset,\n",
        "        test_dataset,\n",
        "        max_epochs=50,\n",
        "        evaluate_every=5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This method implements the training loop using train_dataset and\n",
        "        evaluetes the valid_dataset every evaluate_every epochs. At the end\n",
        "        it tests the classifiers performance based on test_dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_dataset : TF Dataset\n",
        "            Training dataset which consists of pairs of input and labels.\n",
        "        valid_dataset : TF Dataset\n",
        "            Validation dataset which consists of pairs of input and labels.\n",
        "        test_dataset : TF Dataset\n",
        "            Test dataset which consists of pairs of input and labels.\n",
        "        max_epochs : int, optional\n",
        "            Maximum number of times train_dataset is iterated over.\n",
        "            The default is 50.\n",
        "        evaluate_every : int, optional\n",
        "            Every evaluate_every'th epoch the model is evaluated based on\n",
        "            valid_dataset. The default is 5.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "\n",
        "        \"\"\"\n",
        "        for epoch in range(1, max_epochs + 1):\n",
        "            self._reset_train_metrics()\n",
        "            for images, labels in train_dataset:\n",
        "                self._train_step(images, labels)\n",
        "\n",
        "            if epoch % evaluate_every == 0:\n",
        "                self._reset_valid_metrics()\n",
        "                for images, labels in valid_dataset:\n",
        "                    self._valid_step(images, labels)\n",
        "\n",
        "                self._print_train_and_valid_results(epoch, max_epochs)\n",
        "                self._update_tensorboard(epoch)\n",
        "\n",
        "                self._early_stopping_counter.update(self._valid_accuracy.result())\n",
        "                if self._early_stopping_counter.is_stopping_criteria_reached():\n",
        "                    print(\"Early stopping at epoch {:2d}.\".format(epoch))\n",
        "                    self._early_stopping_counter.reset()\n",
        "                    break\n",
        "\n",
        "        self._reset_test_metrics()\n",
        "        for images, labels in test_dataset:\n",
        "            self._test_step(images, labels)\n",
        "\n",
        "        self._print_test_results()\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, images, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self._model(images, training=True)\n",
        "            loss = self._loss(labels, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, self._model.trainable_variables)\n",
        "        self._optimizer.apply_gradients(zip(gradients, self._model.trainable_variables))\n",
        "        self._update_train_metrics(loss, labels, predictions)\n",
        "\n",
        "    @tf.function\n",
        "    def _valid_step(self, images, labels):\n",
        "        predictions = self._model(images, training=False)\n",
        "        loss = self._loss(labels, predictions)\n",
        "        self._update_valid_metrics(loss, labels, predictions)\n",
        "\n",
        "    @tf.function\n",
        "    def _test_step(self, images, labels):\n",
        "        predictions = self._model(images, training=False)\n",
        "        loss = self._loss(labels, predictions)\n",
        "        self._update_test_metrics(loss, labels, predictions)\n",
        "\n",
        "    def _init_metrics(self):\n",
        "        self._init_train_metrics()\n",
        "        self._init_valid_metrics()\n",
        "        self._init_test_metrics()\n",
        "\n",
        "    def _init_train_metrics(self):\n",
        "        self._train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "        self._train_accuracy = tf.keras.metrics.CategoricalAccuracy(\n",
        "            name=\"train_accuracy\"\n",
        "        )\n",
        "\n",
        "    def _init_valid_metrics(self):\n",
        "        self._valid_loss = tf.keras.metrics.Mean(name=\"valid_loss\")\n",
        "        self._valid_accuracy = tf.keras.metrics.CategoricalAccuracy(\n",
        "            name=\"valid_accuracy\"\n",
        "        )\n",
        "\n",
        "    def _init_test_metrics(self):\n",
        "        self._test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "        self._test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
        "\n",
        "    def _reset_train_metrics(self):\n",
        "        self._train_loss.reset_state()\n",
        "        self._train_accuracy.reset_state()\n",
        "\n",
        "    def _reset_valid_metrics(self):\n",
        "        self._valid_loss.reset_state()\n",
        "        self._valid_accuracy.reset_state()\n",
        "\n",
        "    def _reset_test_metrics(self):\n",
        "        self._test_loss.reset_state()\n",
        "        self._test_accuracy.reset_state()\n",
        "\n",
        "    def _update_train_metrics(self, loss, labels, predictions):\n",
        "        self._train_loss(loss)\n",
        "        self._train_accuracy(labels, predictions)\n",
        "\n",
        "    def _update_valid_metrics(self, loss, labels, predictions):\n",
        "        self._valid_loss(loss)\n",
        "        self._valid_accuracy(labels, predictions)\n",
        "\n",
        "    def _update_test_metrics(self, loss, labels, predictions):\n",
        "        self._test_loss(loss)\n",
        "        self._test_accuracy(labels, predictions)\n",
        "\n",
        "    def _print_train_and_valid_results(self, epoch, max_epochs):\n",
        "        print(\n",
        "            \"Epoch {:3d} of {:3d}, \".format(epoch, max_epochs),\n",
        "            \"Train Loss: {:3.3f}, \".format(self._train_loss.result()),\n",
        "            \"Train Accuracy: {:3.3f}%, \".format(self._train_accuracy.result() * 100),\n",
        "            \"Valid Loss: {:3.3f}, \".format(self._valid_loss.result()),\n",
        "            \"Valid Accuracy: {:3.3f}%\".format(self._valid_accuracy.result() * 100),\n",
        "        )\n",
        "\n",
        "    def _print_test_results(self):\n",
        "        print(\n",
        "            \"\\nTest Loss: {:3.3f}, \".format(self._test_loss.result()),\n",
        "            \"Test Accuracy: {:3.3f}%\".format(self._test_accuracy.result() * 100),\n",
        "        )\n",
        "\n",
        "    def _init_tensorboard(self, tensorboard_dir):\n",
        "        self._train_summary_writer = tf.summary.create_file_writer(\n",
        "            os.path.join(tensorboard_dir, \"train\")\n",
        "        )\n",
        "        self._valid_summary_writer = tf.summary.create_file_writer(\n",
        "            os.path.join(tensorboard_dir, \"valid\")\n",
        "        )\n",
        "\n",
        "    def _update_tensorboard(self, epoch):\n",
        "        with self._train_summary_writer.as_default():\n",
        "            tf.summary.scalar(\"Train loss\", self._train_loss.result(), step=epoch)\n",
        "            tf.summary.scalar(\n",
        "                \"Train accuracy\", self._train_accuracy.result(), step=epoch\n",
        "            )\n",
        "\n",
        "        with self._valid_summary_writer.as_default():\n",
        "            tf.summary.scalar(\"Valid loss\", self._valid_loss.result(), step=epoch)\n",
        "            tf.summary.scalar(\n",
        "                \"Valid accuracy\", self._valid_accuracy.result(), step=epoch\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIzr_lo-Pzht"
      },
      "source": [
        "## Main\n",
        "In the following cell all objects are created and the training process is launched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncdQqSd-Pzht"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir ./logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyNcO-P_Pzht"
      },
      "outputs": [],
      "source": [
        "TENSORBOARD_PATH = \"./logs\"\n",
        "\n",
        "data_loader = DataLoaderMNIST()\n",
        "model = MyModel(\"MNISTClassifier\")\n",
        "train = Training(model, TENSORBOARD_PATH)\n",
        "\n",
        "train_dataset = data_loader.train_dataset\n",
        "valid_dataset = data_loader.valid_dataset\n",
        "test_dataset = data_loader.test_dataset\n",
        "\n",
        "train(train_dataset, valid_dataset, test_dataset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}